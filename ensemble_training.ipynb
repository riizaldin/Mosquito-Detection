{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f44aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gdown\n",
    "\n",
    "# Download dataset dari Google Drive\n",
    "file_id = '1KP0OjF7MkeOaOGQDdqPx2AuHk8s9stV3'\n",
    "!gdown --id $file_id --output dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file = 'dataset.zip'\n",
    "extract_dir = '.' # Extract to the current directory\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"'{zip_file}' has been unzipped to '{extract_dir}'\")\n",
    "\n",
    "# Verify the 'DATASET' directory\n",
    "dataset_path = os.path.join(extract_dir, 'DATASET')\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"'{dataset_path}' directory created successfully.\")\n",
    "    print(\"Contents of 'DATASET' directory:\")\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        level = root.replace(dataset_path, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{subindent}{f}')\n",
    "else:\n",
    "    print(f\"Error: '{dataset_path}' directory was not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5922ab6",
   "metadata": {},
   "source": [
    "# Ensemble Model Training: Swin-T + EfficientNetV2-S + ConvNeXt-T\n",
    "\n",
    "Notebook ini menggunakan **Ensemble Learning** dengan 3 model state-of-the-art:\n",
    "1. **Swin Transformer Tiny** (Vision Transformer)\n",
    "2. **EfficientNetV2-S** (CNN Efficient)\n",
    "3. **ConvNeXt Tiny** (Modern CNN)\n",
    "\n",
    "Plus **Test Time Augmentation (TTA)** untuk boost akurasi maksimal.\n",
    "\n",
    "Dataset: Klasifikasi 3 jenis nyamuk\n",
    "- Aedes aegypti\n",
    "- Aedes albopictus  \n",
    "- Culex quinquefasciatus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc34416",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5579eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e58aec",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Config\n",
    "DATA_DIR = 'DATASET'\n",
    "BATCH_SIZE = 16  # Lebih kecil karena 3 model\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.0005\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensemble config\n",
    "ENSEMBLE_WEIGHTS = [0.35, 0.35, 0.30]  # Swin, EfficientNet, ConvNeXt\n",
    "TTA_ENABLE = True\n",
    "TTA_TRANSFORMS = 5  # Jumlah augmentasi untuk TTA\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENSEMBLE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Models: Swin-T + EfficientNetV2-S + ConvNeXt-T\")\n",
    "print(f\"Ensemble Weights: {ENSEMBLE_WEIGHTS}\")\n",
    "print(f\"TTA: {'Enabled' if TTA_ENABLE else 'Disabled'} ({TTA_TRANSFORMS} transforms)\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"LR: {LEARNING_RATE}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bce2b",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b083531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training dengan augmentasi kuat\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transform validation/test\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'test'), transform=val_transform)\n",
    "\n",
    "# Split train -> train + val\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(f\"\\nClasses: {class_names}\")\n",
    "print(f\"Train: {len(train_subset)} | Val: {len(val_subset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57b80d",
   "metadata": {},
   "source": [
    "## 4. Build Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfca633",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "# Model 1: Swin Transformer Tiny\n",
    "print(\"Loading Swin Transformer Tiny...\")\n",
    "swin_model = models.swin_t(weights='DEFAULT')\n",
    "for name, param in swin_model.named_parameters():\n",
    "    if 'features.0' in name or 'features.1' in name or 'features.2' in name:\n",
    "        param.requires_grad = False\n",
    "swin_model.head = nn.Sequential(\n",
    "    nn.Linear(swin_model.head.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes)\n",
    ")\n",
    "swin_model = swin_model.to(DEVICE)\n",
    "\n",
    "# Model 2: EfficientNetV2-S\n",
    "print(\"Loading EfficientNetV2-S...\")\n",
    "efficientnet_model = models.efficientnet_v2_s(weights='DEFAULT')\n",
    "for param in list(efficientnet_model.parameters())[:-30]:\n",
    "    param.requires_grad = False\n",
    "efficientnet_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(efficientnet_model.classifier[1].in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes)\n",
    ")\n",
    "efficientnet_model = efficientnet_model.to(DEVICE)\n",
    "\n",
    "# Model 3: ConvNeXt Tiny\n",
    "print(\"Loading ConvNeXt Tiny...\")\n",
    "convnext_model = models.convnext_tiny(weights='DEFAULT')\n",
    "for param in list(convnext_model.parameters())[:-30]:\n",
    "    param.requires_grad = False\n",
    "convnext_model.classifier[2] = nn.Sequential(\n",
    "    nn.Linear(convnext_model.classifier[2].in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes)\n",
    ")\n",
    "convnext_model = convnext_model.to(DEVICE)\n",
    "\n",
    "models_list = [swin_model, efficientnet_model, convnext_model]\n",
    "model_names = ['Swin-T', 'EfficientNetV2-S', 'ConvNeXt-T']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENSEMBLE MODELS LOADED\")\n",
    "print(\"=\" * 60)\n",
    "for i, (model, name) in enumerate(zip(models_list, model_names)):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{i+1}. {name:20s} | Total: {total:,} | Trainable: {trainable:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80c10a",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_epoch(models, criterion, optimizers, data_loader, device, weights):\n",
    "    \"\"\"Train ensemble satu epoch\"\"\"\n",
    "    for model in models:\n",
    "        model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass semua model\n",
    "        ensemble_outputs = []\n",
    "        for model, optimizer in zip(models, optimizers):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            ensemble_outputs.append(outputs)\n",
    "        \n",
    "        # Weighted average ensemble\n",
    "        combined_output = sum(w * out for w, out in zip(weights, ensemble_outputs))\n",
    "        \n",
    "        # Loss dan backward\n",
    "        loss = criterion(combined_output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update semua optimizer\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(combined_output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(data_loader), 100 * correct / total\n",
    "\n",
    "def validate_ensemble(models, criterion, data_loader, device, weights):\n",
    "    \"\"\"Validate ensemble\"\"\"\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            ensemble_outputs = [model(inputs) for model in models]\n",
    "            combined_output = sum(w * out for w, out in zip(weights, ensemble_outputs))\n",
    "            \n",
    "            loss = criterion(combined_output, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(combined_output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(data_loader), 100 * correct / total\n",
    "\n",
    "print(\"Training functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef40d63",
   "metadata": {},
   "source": [
    "## 6. Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss dan optimizer untuk setiap model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizers = [optim.Adam(model.parameters(), lr=LEARNING_RATE) for model in models_list]\n",
    "schedulers = [optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', factor=0.5, patience=5) \n",
    "              for opt in optimizers]\n",
    "\n",
    "# History\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MULAI TRAINING ENSEMBLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_ensemble_epoch(\n",
    "        models_list, criterion, optimizers, train_loader, DEVICE, ENSEMBLE_WEIGHTS\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_ensemble(\n",
    "        models_list, criterion, val_loader, DEVICE, ENSEMBLE_WEIGHTS\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Update LR\n",
    "    for scheduler in schedulers:\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] - \"\n",
    "          f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
    "          f\"Acc: {train_acc:.2f}%/{val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        for i, (model, name) in enumerate(zip(models_list, model_names)):\n",
    "            torch.save(model.state_dict(), f'best_{name.lower().replace(\"-\", \"_\")}.pth')\n",
    "        print(f\"  â†’ Models saved (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTraining selesai! Best Val Acc: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a0ef8",
   "metadata": {},
   "source": [
    "## 7. Test Time Augmentation (TTA) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tta_transforms(n=5):\n",
    "    \"\"\"Generate TTA transforms\"\"\"\n",
    "    tta_list = [\n",
    "        # Original\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        # Horizontal flip\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        # Vertical flip\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomVerticalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        # Rotation +10\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation((10, 10)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        # Rotation -10\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation((-10, -10)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    ]\n",
    "    return tta_list[:n]\n",
    "\n",
    "def predict_with_tta(models, image, tta_transforms, weights, device):\n",
    "    \"\"\"Predict dengan TTA\"\"\"\n",
    "    all_predictions = []\n",
    "    \n",
    "    for transform in tta_transforms:\n",
    "        img_transformed = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        ensemble_outputs = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(img_transformed)\n",
    "                ensemble_outputs.append(output)\n",
    "        \n",
    "        # Weighted average\n",
    "        combined = sum(w * out for w, out in zip(weights, ensemble_outputs))\n",
    "        all_predictions.append(torch.softmax(combined, dim=1))\n",
    "    \n",
    "    # Average semua TTA predictions\n",
    "    final_pred = torch.mean(torch.stack(all_predictions), dim=0)\n",
    "    return final_pred\n",
    "\n",
    "print(f\"TTA transforms ready: {TTA_TRANSFORMS} augmentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e485abb",
   "metadata": {},
   "source": [
    "## 8. Evaluate Ensemble + TTA on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac83154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best models\n",
    "for i, (model, name) in enumerate(zip(models_list, model_names)):\n",
    "    model.load_state_dict(torch.load(f'best_{name.lower().replace(\"-\", \"_\")}.pth'))\n",
    "    model.eval()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUASI TEST SET DENGAN ENSEMBLE + TTA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "if TTA_ENABLE:\n",
    "    tta_transforms = get_tta_transforms(TTA_TRANSFORMS)\n",
    "    \n",
    "    # Evaluasi per image dengan TTA\n",
    "    test_dataset_pil = datasets.ImageFolder(os.path.join(DATA_DIR, 'test'))\n",
    "    \n",
    "    for img, label in tqdm(test_dataset_pil, desc=\"Testing with TTA\"):\n",
    "        pred = predict_with_tta(models_list, img, tta_transforms, ENSEMBLE_WEIGHTS, DEVICE)\n",
    "        all_preds.append(torch.argmax(pred, dim=1).item())\n",
    "        all_labels.append(label)\n",
    "else:\n",
    "    # Evaluasi tanpa TTA (standard ensemble)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            ensemble_outputs = [model(inputs) for model in models_list]\n",
    "            combined = sum(w * out for w, out in zip(ENSEMBLE_WEIGHTS, ensemble_outputs))\n",
    "            _, predicted = torch.max(combined.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"HASIL EVALUASI\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"Precision:      {precision:.4f}\")\n",
    "print(f\"Recall:         {recall:.4f}\")\n",
    "print(f\"F1-Score:       {f1:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e01ba4",
   "metadata": {},
   "source": [
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "axes[0].plot(train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, 'r-', label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontweight='bold')\n",
    "axes[0].set_title('Ensemble Training/Validation Loss', fontweight='bold', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accs, 'b-', label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(val_accs, 'r-', label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "axes[1].set_title('Ensemble Training/Validation Accuracy', fontweight='bold', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=class_names, yticklabels=class_names,\n",
    "           ax=axes[0], annot_kws={'size': 14, 'weight': 'bold'})\n",
    "axes[0].set_xlabel('Predicted', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('True', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix (Count)', fontweight='bold', fontsize=14)\n",
    "\n",
    "cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Greens',\n",
    "           xticklabels=class_names, yticklabels=class_names,\n",
    "           ax=axes[1], annot_kws={'size': 14, 'weight': 'bold'})\n",
    "axes[1].set_xlabel('Predicted', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('True', fontweight='bold', fontsize=12)\n",
    "axes[1].set_title('Confusion Matrix (%)', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd8f9b",
   "metadata": {},
   "source": [
    "## 10. Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP, TN, FP, FN per class\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAIL CONFUSION MATRIX (TP, TN, FP, FN)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tp_tn_data = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    tp_tn_data.append({\n",
    "        'Class': class_name,\n",
    "        'TP': TP,\n",
    "        'TN': TN,\n",
    "        'FP': FP,\n",
    "        'FN': FN\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  TP: {TP} | TN: {TN} | FP: {FP} | FN: {FN}\")\n",
    "\n",
    "tp_tn_df = pd.DataFrame(tp_tn_data)\n",
    "display(tp_tn_df)\n",
    "\n",
    "# Metrics table\n",
    "metrics_data = [{\n",
    "    'Class': 'Overall',\n",
    "    'Accuracy': f\"{accuracy*100:.2f}%\",\n",
    "    'Precision': f\"{precision:.4f}\",\n",
    "    'Recall': f\"{recall:.4f}\",\n",
    "    'F1-Score': f\"{f1:.4f}\",\n",
    "    'Support': len(all_labels)\n",
    "}]\n",
    "\n",
    "for class_name in class_names:\n",
    "    metrics_data.append({\n",
    "        'Class': class_name,\n",
    "        'Accuracy': '-',\n",
    "        'Precision': f\"{report[class_name]['precision']:.4f}\",\n",
    "        'Recall': f\"{report[class_name]['recall']:.4f}\",\n",
    "        'F1-Score': f\"{report[class_name]['f1-score']:.4f}\",\n",
    "        'Support': int(report[class_name]['support'])\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072160e0",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e92b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'ensemble_info': {\n",
    "        'models': model_names,\n",
    "        'weights': ENSEMBLE_WEIGHTS,\n",
    "        'tta_enabled': TTA_ENABLE,\n",
    "        'tta_transforms': TTA_TRANSFORMS if TTA_ENABLE else 0,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'device': str(DEVICE)\n",
    "    },\n",
    "    'classes': class_names,\n",
    "    'training_history': {\n",
    "        'train_losses': [float(x) for x in train_losses],\n",
    "        'val_losses': [float(x) for x in val_losses],\n",
    "        'train_accuracies': [float(x) for x in train_accs],\n",
    "        'val_accuracies': [float(x) for x in val_accs],\n",
    "        'best_val_accuracy': float(best_val_acc)\n",
    "    },\n",
    "    'test_metrics': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1)\n",
    "    },\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'classification_report': report\n",
    "}\n",
    "\n",
    "with open('ensemble_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HASIL AKHIR ENSEMBLE + TTA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Models: {', '.join(model_names)}\")\n",
    "print(f\"Ensemble Weights: {ENSEMBLE_WEIGHTS}\")\n",
    "print(f\"TTA: {'Yes' if TTA_ENABLE else 'No'} ({TTA_TRANSFORMS if TTA_ENABLE else 0} augmentations)\")\n",
    "print(f\"\\nBest Val Acc: {best_val_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - best_swin_t.pth\")\n",
    "print(\"  - best_efficientnetv2_s.pth\")\n",
    "print(\"  - best_convnext_t.pth\")\n",
    "print(\"  - ensemble_training_history.png\")\n",
    "print(\"  - ensemble_confusion_matrix.png\")\n",
    "print(\"  - ensemble_results.json\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
